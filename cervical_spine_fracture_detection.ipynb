{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sajjkavinda/cervical-spine-fracture-detection/blob/main/cervical_spine_fracture_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cervical Spine Fracture Detection**\n",
        "\n",
        "This notebook demonstrate the process using deep learning to detect and predict human cervical spine fractures from medical imaging data. This project tend to explore a combination of deep learning and computer vision techniques for medical diagnosis, including preprocessing the dataset, model training and testing, evaluation, and prediction.\n",
        "\n",
        "Dataset Source: https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection\n"
      ],
      "metadata": {
        "id": "xyNlbflBWSk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Original dataset is very large and needs high computational power to process. I've extracted a subset of the dataset using the kaggle notebook.\n",
        "Please refer here for more details: https://github.com/sajjkavinda/cervical-spine-fracture-detection/blob/dataset-subesetting/rsna-subsetting.ipynb"
      ],
      "metadata": {
        "id": "Nu34FSk1sGTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1**\n",
        "\n",
        "\n",
        "*   Mount the google drive (the zip file is stored in the google drive)\n",
        "*   Unzip the zip files to be used in here\n",
        "\n",
        "\n",
        "*   Randomly check the availability of the records\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yRHDP_ottOCy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx8VrQmNDntG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2n4KsYzE2zh"
      },
      "outputs": [],
      "source": [
        "!unzip -q \"/content/drive/MyDrive/Dataset-RSNA/rsna_subset.zip\" -d \"/content/rsna_subset\" && echo \"Unzip completed successfully!\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"/content/rsna_subset/rsna_subset_images\")[:5]"
      ],
      "metadata": {
        "id": "1bILEzdkCG1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2**\n",
        "\n",
        "\n",
        "*   Import the necessary libraries to be used.\n",
        "\n",
        "\n",
        "*   Install necessary python packages to be used.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8R7T_gVlucNu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvuR5bFOFD_9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import pydicom\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhRoaRcYHVoG"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y gdcm python-gdcm pydicom pylibjpeg pylibjpeg-libjpeg pylibjpeg-openjpeg --quiet\n",
        "!pip install pydicom pylibjpeg pylibjpeg-libjpeg pylibjpeg-openjpeg --quiet\n",
        "!pip install opencv-python matplotlib tqdm --quiet\n",
        "!pip install pandas scikit-learn matplotlib tqdm --quiet\n",
        "!pip install torch torchvision torchaudio --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 3**\n",
        "\n",
        "\n",
        "*   I have converted the dataset from DCM (Digital Imaging and Communications in Medicine) format to PNG format, to be familiar with the dataset.\n",
        "\n",
        "\n",
        "*   This part of the code iterate through every file (dcm) and converts them to png format and stores inside the folder rsna_png_images.\n",
        "\n",
        "\n",
        "\n",
        "> Usage of the libraries\n",
        "\n",
        "*   CV2: Image normalization and saving as PNG files.\n",
        "*   pydicom: Reading DICOM format.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WxPaDmIbvPN7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBJKk92yHag1"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "input_dir = \"/content/rsna_subset/rsna_subset_images\"\n",
        "output_dir = \"/content/rsna_png_images\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Iterate through every patient\n",
        "for patient in tqdm(os.listdir(input_dir)):\n",
        "    patient_path = os.path.join(input_dir, patient)\n",
        "    if not os.path.isdir(patient_path):\n",
        "        continue\n",
        "    patient_output = os.path.join(output_dir, patient)\n",
        "    os.makedirs(patient_output, exist_ok=True)\n",
        "\n",
        "    # Converting the dcm files to png\n",
        "    for file in os.listdir(patient_path):\n",
        "        if file.endswith(\".dcm\"):\n",
        "            dcm_path = os.path.join(patient_path, file)\n",
        "            png_path = os.path.join(patient_output, file.replace(\".dcm\", \".png\"))\n",
        "\n",
        "            try:\n",
        "                dcm = pydicom.dcmread(dcm_path)\n",
        "                img = dcm.pixel_array\n",
        "                img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)\n",
        "                cv2.imwrite(png_path, img)\n",
        "            except Exception as e: # error handling\n",
        "                print(f\"Error converting {dcm_path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4**\n",
        "\n",
        "\n",
        "\n",
        "*   Check for the number of data available for training, testing and validation.\n",
        "\n",
        "\n",
        "*   Print a sample of fractured and normal item from the dataset"
      ],
      "metadata": {
        "id": "TBYHdMY6eTLd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4WqWopVIlGs"
      },
      "outputs": [],
      "source": [
        "# Get available patient ids\n",
        "labels = pd.read_csv(\"/content/rsna_subset/subset_csv/train.csv\")\n",
        "available_uids = set(os.listdir(\"/content/rsna_png_images\"))\n",
        "\n",
        "labels_existing = labels[labels[\"StudyInstanceUID\"].isin(available_uids)]\n",
        "\n",
        "# Separate fractured and normal\n",
        "fractured_cases = labels_existing[labels_existing[\"patient_overall\"] == 1]\n",
        "normal_cases = labels_existing[labels_existing[\"patient_overall\"] == 0]\n",
        "\n",
        "print(f\"Available fractured: {len(fractured_cases)}, normal: {len(normal_cases)}\")\n",
        "\n",
        "# Pick one of each\n",
        "fractured_uid = fractured_cases[\"StudyInstanceUID\"].iloc[0]\n",
        "normal_uid = normal_cases[\"StudyInstanceUID\"].iloc[0]\n",
        "\n",
        "print(\"Fractured Study UID:\", fractured_uid)\n",
        "print(\"Normal Study UID:\", normal_uid)\n",
        "\n",
        "# Display Images\n",
        "def show_example(study_uid, label):\n",
        "    study_path = os.path.join(\"/content/rsna_png_images\", study_uid)\n",
        "    files = sorted([f for f in os.listdir(study_path) if f.endswith(\".png\")])\n",
        "\n",
        "    if not files:\n",
        "        print(f\"No images found for {study_uid}\")\n",
        "        return\n",
        "\n",
        "    img_path = os.path.join(study_path, files[len(files)//2]) # taking the middle slice\n",
        "    img = Image.open(img_path)\n",
        "\n",
        "    plt.imshow(img, cmap=\"gray\")\n",
        "    plt.title(f\"{label} â€” {study_uid}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "show_example(fractured_uid, \"Fractured\")\n",
        "show_example(normal_uid, \"Normal\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5**\n",
        "\n",
        "\n",
        "\n",
        "*   Prepare the dataset for train the model.\n",
        "*   Transform is used to resize and normalize the images.\n",
        "*   Batch sizes for train and validation process are defined here.\n",
        "\n"
      ],
      "metadata": {
        "id": "J0hjYXodgOsZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V4xSQeNsI06"
      },
      "outputs": [],
      "source": [
        "class SpineDataset(Dataset):\n",
        "    def __init__(self, df, base_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.base_dir = base_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        study_uid = row[\"StudyInstanceUID\"]\n",
        "        label = row[\"patient_overall\"]\n",
        "\n",
        "        folder = os.path.join(self.base_dir, study_uid)\n",
        "        files = [f for f in os.listdir(folder) if f.endswith(\".png\")]\n",
        "        img_path = os.path.join(folder, files[len(files)//2])  # middle slice\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "base_dir = \"/content/rsna_png_images\"\n",
        "train_df = labels_existing.sample(frac=0.8, random_state=42)\n",
        "val_df = labels_existing.drop(train_df.index)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_dataset = SpineDataset(train_df, base_dir, transform)\n",
        "val_dataset = SpineDataset(val_df, base_dir, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6**\n",
        "\n",
        "\n",
        "\n",
        "*   Select the appropriate hardware to train the model.\n",
        "*   cuda: NVIDIA GPU, mps: Apple GPU, cpu: If GPU isn't available\n",
        "*   Load the pre-trained ResNet18 model.\n",
        "*   Define loss function and optimizer.\n",
        "\n"
      ],
      "metadata": {
        "id": "zEhoMUoYg7fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = models.resnet18(pretrained=True)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "id": "tN2HIU6y-4Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 7**\n",
        "\n",
        "\n",
        "\n",
        "*   Define number of Epochs\n",
        "*   Iterate through the number of epochs and loop over batches.\n",
        "*   Clear any previously computed gradients.\n",
        "*   Send the batch of images through the model to get predicted outputs.\n",
        "*   Measure how far the predicted outputs are from the true labels using.\n",
        "*   Calculate gradients of the loss with respect to each model parameter.\n",
        "*   Applie the optimizer to adjust the model parameters using the computed gradients.\n",
        "*   Add the batch loss to a running total to later compute the average epoch loss."
      ],
      "metadata": {
        "id": "z8BiSMR-ipUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Train loss: {total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "AxKXFjaT-7e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "predictions, truths = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "        preds = torch.argmax(outputs, 1).cpu().numpy()\n",
        "        predictions.extend(preds)\n",
        "        truths.extend(labels.numpy())\n",
        "\n",
        "acc = np.mean(np.array(predictions) == np.array(truths))\n",
        "print(f\"Validation Accuracy: {acc:.2%}\")\n"
      ],
      "metadata": {
        "id": "IwDzi_OE-_GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def denormalize(img_tensor):\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    img = img_tensor * std + mean\n",
        "    img = torch.clamp(img, 0, 1)\n",
        "    return img\n",
        "\n",
        "def show_predictions(dataset, model, device, label_names=[\"Normal\", \"Fractured\"]):\n",
        "    model.eval()\n",
        "    shown = {\"pos\": False, \"neg\": False}\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        img, label = dataset[i]\n",
        "        with torch.no_grad():\n",
        "            pred = model(img.unsqueeze(0).to(device))\n",
        "            pred_label = torch.argmax(pred, 1).item()\n",
        "\n",
        "        # Pick one predicted positive and one predicted negative\n",
        "        if (pred_label == 1 and not shown[\"pos\"]) or (pred_label == 0 and not shown[\"neg\"]):\n",
        "            img_vis = denormalize(img)\n",
        "            plt.imshow(np.transpose(img_vis.numpy(), (1, 2, 0)))\n",
        "            plt.title(f\"Predicted: {label_names[pred_label]} (True: {label_names[label]})\")\n",
        "            plt.axis(\"off\")\n",
        "            plt.show()\n",
        "\n",
        "            if pred_label == 1:\n",
        "                shown[\"pos\"] = True\n",
        "            else:\n",
        "                shown[\"neg\"] = True\n",
        "\n",
        "        if all(shown.values()):\n",
        "            break\n",
        "\n",
        "show_predictions(val_dataset, model, device)\n"
      ],
      "metadata": {
        "id": "KUOimqLx_FAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"rsna_png_images\"  # folder containing image folders\n",
        "train_csv = \"/content/rsna_subset/subset_csv/train.csv\"\n",
        "test_csv = \"/content/rsna_subset/subset_csv/test.csv\"\n",
        "\n",
        "# Load metadata\n",
        "train_df = pd.read_csv(train_csv)\n",
        "test_df = pd.read_csv(test_csv)\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)"
      ],
      "metadata": {
        "id": "JOTXAQTEDX4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your PNG images\n",
        "data_dir = \"rsna_png_images\"\n",
        "\n",
        "# Only keep studies that exist in the folder\n",
        "available_uids = set(os.listdir(data_dir))\n",
        "train_df = train_df[train_df[\"StudyInstanceUID\"].isin(available_uids)]\n",
        "val_df = val_df[val_df[\"StudyInstanceUID\"].isin(available_uids)]\n",
        "\n",
        "print(\"Train studies available:\", len(train_df))\n",
        "print(\"Validation studies available:\", len(val_df))\n",
        "\n",
        "test_df = test_df[test_df[\"StudyInstanceUID\"].isin(available_uids)]\n",
        "print(\"Test studies available:\", len(test_df))\n"
      ],
      "metadata": {
        "id": "OULndTWyFq7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset processing\n",
        "class RSNADataset(Dataset):\n",
        "    def __init__(self, df, data_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        study_uid = row[\"StudyInstanceUID\"]\n",
        "        label = int(row[\"patient_overall\"])\n",
        "\n",
        "        folder = os.path.join(self.data_dir, study_uid)\n",
        "        files = [f for f in os.listdir(folder) if f.endswith(\".png\")]\n",
        "        img_path = os.path.join(folder, files[len(files)//2])  # middle slice\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "x0-0WFEBDYta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform\n",
        "train_df, val_df = train_test_split(\n",
        "    train_df,\n",
        "    test_size=0.2,\n",
        "    stratify=train_df[\"patient_overall\"],  # use correct label column\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = RSNADataset(train_df, data_dir, transform)\n",
        "val_dataset = RSNADataset(val_df, data_dir, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
      ],
      "metadata": {
        "id": "YjDd_QZBDb1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load pre-trained ResNet18\n",
        "weights = ResNet18_Weights.DEFAULT\n",
        "model = resnet18(weights=weights)\n",
        "\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "\n",
        "# Move to device\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "RvPgeL5fDgzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss, correct, total = 0, 0, 0\n",
        "    for imgs, labels in tqdm(train_loader):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += preds.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    val_correct, val_total = 0, 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            _, preds = outputs.max(1)\n",
        "            val_correct += preds.eq(labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Acc: {100*correct/total:.2f}% | Val Acc: {100*val_correct/val_total:.2f}%\")\n"
      ],
      "metadata": {
        "id": "4it8qB3lDj5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the accuracy of the model\n",
        "test_dataset = RSNADataset(test_df, data_dir, transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "test_correct, test_total = 0, 0\n",
        "for imgs, labels in test_loader:\n",
        "    imgs, labels = imgs.to(device), labels.to(device)\n",
        "    outputs = model(imgs)\n",
        "    _, preds = outputs.max(1)\n",
        "    test_correct += preds.eq(labels).sum().item()\n",
        "    test_total += labels.size(0)\n",
        "\n",
        "print(f\"est Accuracy: {100*test_correct/test_total:.2f}%\")\n"
      ],
      "metadata": {
        "id": "D-8teoaKDkjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample prediction using trained model\n",
        "def show_random_predictions(model, dataset, n=2):\n",
        "    model.eval()\n",
        "    indices = random.sample(range(len(dataset)), n)  # pick n random images\n",
        "    fig, axes = plt.subplots(1, n, figsize=(10, 5))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, label = dataset[idx]\n",
        "        with torch.no_grad():\n",
        "            pred = model(img.unsqueeze(0).to(device))\n",
        "            pred_label = torch.argmax(pred, dim=1).item()\n",
        "\n",
        "        img_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "        img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
        "\n",
        "        axes[i].imshow(img_np)\n",
        "        axes[i].set_title(f\"Pred: {'Fractured' if pred_label==1 else 'Normal'} | True: {'Fractured' if label==1 else 'Normal'}\")\n",
        "        axes[i].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "show_random_predictions(model, val_dataset, n=2)"
      ],
      "metadata": {
        "id": "9fEVjmW7V501"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOXe5mpITIa6E1TTpTdi68r",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}